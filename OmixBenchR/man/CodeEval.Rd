% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/CodeEval.R
\name{CodeEval}
\alias{CodeEval}
\title{Evaluate Bioinformatics R Code Using LLM}
\usage{
CodeEval(
  task,
  response,
  llm_client,
  max_retries = 3,
  schema_strict = TRUE,
  schema_type = "auto",
  verbose = FALSE
)
}
\arguments{
\item{task}{A character string containing the bioinformatics task or question that the code
is supposed to solve. This provides context for the evaluation and helps the LLM assess
whether the code addresses the biological question appropriately.}

\item{response}{A character string containing the R code to be evaluated. This should be
the complete code solution that attempts to solve the given bioinformatics task.}

\item{llm_client}{An LLM provider object created by functions like \code{llm_openai()} or
\code{llm_ollama()}. This object contains the configuration for connecting to and
communicating with the specific LLM service that will perform the evaluation.}

\item{max_retries}{Integer. Maximum number of retry attempts if the LLM fails to provide
a valid JSON response (default: 3). The function will retry if JSON parsing fails or
if the response doesn't match the expected schema structure.}

\item{schema_strict}{Logical. Whether to enforce strict JSON schema validation (default: TRUE).
When TRUE, the LLM response must exactly match the predefined evaluation schema with no
additional properties allowed. When FALSE, allows more flexible JSON structure.}

\item{schema_type}{Character. Method for enforcing JSON response format (default: "auto").
Options include:
\itemize{
\item "auto": Automatically detect best method based on LLM provider
\item "text-based": Add JSON instructions to prompt (works with any provider)
\item "openai": Use OpenAI's native JSON mode (requires compatible OpenAI API)
\item "ollama": Use Ollama's native JSON mode (requires compatible Ollama model)
\item "openai_oo": OpenAI mode without schema enforcement in API
\item "ollama_oo": Ollama mode without schema enforcement in API
}}

\item{verbose}{Logical. Whether to print detailed interaction logs to console (default: FALSE).
When TRUE, shows the prompt being sent, the LLM's response, and any retry attempts.
Useful for debugging and monitoring the evaluation process.}
}
\value{
A named list containing the evaluation results, or a list with an error message if
evaluation fails. The successful return structure includes:
\itemize{
\item \code{total_score}: Integer (0-100) representing the overall evaluation score
\item \code{breakdown}: Named list with individual dimension scores:
\itemize{
\item \code{problem_solving}: Score 0-20 for addressing the biological question
\item \code{technical_implementation}: Score 0-25 for appropriate methods and workflow
\item \code{data_handling}: Score 0-15 for preprocessing and quality control
\item \code{statistical_rigor}: Score 0-15 for statistical methods and corrections
\item \code{domain_knowledge}: Score 0-10 for biological context understanding
\item \code{robustness}: Score 0-10 for error handling and validation
\item \code{documentation}: Score 0-5 for code clarity and documentation
}
}
If evaluation fails, returns \code{list(error = "error message")}.
}
\description{
This function evaluates bioinformatics R code solutions using a Language Learning Model (LLM)
based on scientific correctness, technical quality, and professional standards. It returns
a structured JSON assessment with detailed scoring across multiple dimensions.
}
\details{
This function implements a comprehensive evaluation framework specifically designed for
bioinformatics code assessment. The evaluation covers three main areas:

\strong{Core Scientific Validity (45 points):}
\itemize{
\item Problem solving approach and biological question addressing
\item Technical implementation with appropriate bioinformatics methods
\item Complete analysis workflows with logical flow
}

\strong{Technical Quality (30 points):}
\itemize{
\item Data handling including preprocessing and quality control
\item Statistical rigor with proper corrections and methods
}

\strong{Professional Excellence (25 points):}
\itemize{
\item Domain knowledge and biological interpretation
\item Code robustness and error handling
\item Documentation and usability
}

The function uses a positive scoring system where points are awarded for good practices
rather than deducted for shortcomings. This encourages comprehensive evaluation and
recognizes partial solutions that demonstrate scientific understanding.

The evaluation leverages the \code{llmhelper::get_llm_response()} function with JSON
schema enforcement to ensure consistent, structured output that can be easily processed
and analyzed programmatically.
}
\examples{
\dontrun{
# Set up LLM client
library(llmhelper)
client <- llm_openai(
  base_url = "https://api.openai.com/v1/chat/completions",
  api_key = Sys.getenv("OPENAI_API_KEY"),
  model = "gpt-4",
  temperature = 0.2
)

# Define a bioinformatics task
task <- "Analyze RNA-seq data to identify differentially expressed genes"

# Example R code to evaluate
code <- "
library(DESeq2)
library(ggplot2)

# Load count data
counts <- read.csv('counts.csv', row.names=1)
coldata <- read.csv('coldata.csv', row.names=1)

# Create DESeq2 object
dds <- DESeqDataSetFromMatrix(countData = counts,
                              colData = coldata,
                              design = ~ condition)

# Run DESeq2 analysis
dds <- DESeq(dds)
res <- results(dds)

# Apply multiple testing correction
res$padj <- p.adjust(res$pvalue, method='BH')

# Filter significant genes
sig_genes <- subset(res, padj < 0.05 & abs(log2FoldChange) > 1)
print(summary(res))
"

# Evaluate the code
evaluation <- CodeEval(task, code, client, verbose = TRUE)

# Print results
cat("Total Score:", evaluation$total_score, "/100\n")
cat("Problem Solving:", evaluation$breakdown$problem_solving, "/20\n")
cat("Technical Implementation:", evaluation$breakdown$technical_implementation, "/25\n")

# Batch evaluation example
tasks <- c("Perform GO enrichment analysis", "Create a volcano plot")
codes <- c("library(clusterProfiler)...", "library(ggplot2)...")

results <- mapply(CodeEval, tasks, codes,
  MoreArgs = list(llm_client = client),
  SIMPLIFY = FALSE
)
}

}
\seealso{
\code{\link[llmhelper]{get_llm_response}} for the underlying LLM communication function,
\code{\link[llmhelper]{llm_openai}} for creating OpenAI-compatible LLM providers,
\code{\link[llmhelper]{llm_ollama}} for creating Ollama LLM providers
}
\author{
Zaoqu Liu; Email: liuzaoqu@163.com
}

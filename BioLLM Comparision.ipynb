{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d353d2eb-a564-4b8c-a56d-78154f7683b1",
   "metadata": {},
   "source": [
    "## BioGPT: Biomedical Text vs Code Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "155658e8-34d4-4423-92c4-8243d03367da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba90b72b-0408-4108-a600-7a02d19cbe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path and load model\n",
    "model_path = \"/mnt/liuzq/modelscope_cache/models/xw-download/AIModel/microsoft/biogpt\"\n",
    "snapshots_path = os.path.join(model_path, \"snapshots\")\n",
    "snapshot_dir = os.listdir(snapshots_path)[0]\n",
    "real_model_path = os.path.join(snapshots_path, snapshot_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1599b2d-2ff9-4258-9569-c4d3f7645fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "tokenizer = AutoTokenizer.from_pretrained(real_model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(real_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1355e90-feee-4a3f-aa02-107170b2e900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: COVID-19 is\n",
      "Output: COVID-19 is a global pandemic.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test biomedical question\n",
    "prompt1 = \"COVID-19 is\"\n",
    "inputs1 = tokenizer(prompt1, return_tensors=\"pt\")\n",
    "outputs1 = model.generate(**inputs1, max_length=10000, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "result1 = tokenizer.decode(outputs1[0], skip_special_tokens=True)\n",
    "print(f\"Prompt: {prompt1}\")\n",
    "print(f\"Output: {result1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da99439f-72fd-494d-8e9a-4c2563616f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Write R code for differential expression analysis using limma package\n",
      "Output: Write R code for differential expression analysis using limma package.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test code generation\n",
    "prompt2 = \"Write R code for differential expression analysis using limma package\"\n",
    "inputs2 = tokenizer(prompt2, return_tensors=\"pt\")\n",
    "outputs2 = model.generate(**inputs2, max_length=10000, num_return_sequences=1, do_sample=True, top_p=0.95, temperature=0.8, pad_token_id=tokenizer.eos_token_id)\n",
    "result2 = tokenizer.decode(outputs2[0], skip_special_tokens=True)\n",
    "print(f\"Prompt: {prompt2}\")\n",
    "print(f\"Output: {result2}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2843024b-bac6-477f-9cef-906306dc1123",
   "metadata": {},
   "source": [
    "## BioMistral: Biomedical Text vs Code Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e696d455-50f0-4f90-81ef-b23696c96053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model path\n",
    "model_path = \"/mnt/liuzq/modelscope_cache/models/xw-download/AIModel/BioMistral/BioMistral-7B/snapshots/9a11e1ffa817c211cbb52ee1fb312dc6b61b40a5/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48eb69fd-1811-44db-a1f7-f160bd11a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8435df1-852d-425b-b358-a7fbd68fca7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: COVID-19 is\n",
      "Output: COVID-19 is a novel coronavirus that was first identified in December 2019 in Wuhan, China. It is a highly infectious disease that has spread rapidly around the world. The World Health Organization (WHO) declared COVID-19 a pandemic on 11 March 2020. As of 15 June 2020, there have been 7,800,000 confirmed cases and 430,000 deaths worldwide .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test biomedical question\n",
    "prompt1 = \"COVID-19 is\"\n",
    "inputs1 = tokenizer(prompt1, return_tensors=\"pt\").to(model.device)\n",
    "outputs1 = model.generate(**inputs1, max_length=10000, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "result1 = tokenizer.decode(outputs1[0], skip_special_tokens=True)\n",
    "print(f\"Prompt: {prompt1}\")\n",
    "print(f\"Output: {result1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3ad0a93-c3c6-45d3-b3bd-c562483fb983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Write R code for differential expression analysis using limma package\n",
      "Output: Write R code for differential expression analysis using limma package. Note the difference between the limma design matrix in this example and the design matrixes in examples 2 and 3. In example 2 and 3, a column of the design matrix was used to indicate the factor of interest for each sample, while in this example, a row of the design matrix is used to indicate the factor of interest for each gene.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test code generation\n",
    "prompt2 = \"Write R code for differential expression analysis using limma package\"\n",
    "inputs2 = tokenizer(prompt2, return_tensors=\"pt\").to(model.device)\n",
    "outputs2 = model.generate(**inputs2, max_length=10000, num_return_sequences=1, do_sample=True, top_p=0.95, temperature=0.8, pad_token_id=tokenizer.eos_token_id)\n",
    "result2 = tokenizer.decode(outputs2[0], skip_special_tokens=True)\n",
    "print(f\"Prompt: {prompt2}\")\n",
    "print(f\"Output: {result2}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29cfd75-0222-40fc-8a92-2013b1b9c323",
   "metadata": {},
   "source": [
    "## BioMedLM: Biomedical Text vs Code Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd9c3e70-a4db-4d11-aadc-0d29b3c7c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/mnt/liuzq/modelscope_cache/models/xw-download/AIModel/stanford-crfm/-BioMedLM/snapshots/3e1a0abb814b8398bc34b4b6680ecf2c26d6a66f/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d1a9157-c7eb-4e7d-b5cd-aa7583d20742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5813895-a4fd-4d16-ab48-219a31d43fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: COVID-19 is\n",
      "Output: COVID-19 is a novel coronavirus, the virus is a major cause of the disease.\n",
      "\n",
      "The authors declare no competing interests.\n",
      "\n",
      "Author contributions {#s0010}\n",
      "=========================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test biomedical question\n",
    "prompt1 = \"COVID-19 is\"\n",
    "inputs1 = tokenizer(prompt1, return_tensors=\"pt\")\n",
    "outputs1 = model.generate(**inputs1, max_length=100, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "result1 = tokenizer.decode(outputs1[0], skip_special_tokens=True)\n",
    "print(f\"Prompt: {prompt1}\")\n",
    "print(f\"Output: {result1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f016d0f-9a8c-471e-8663-103cdd335fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Write R code for differential expression analysis using limma package\n",
      "Output: Write R code for differential expression analysis using limma package (see the [Supplementary Materials and methods. A total of 25,432 of the 25,432 (16.9%) of the 144 total genes were shared between the two datasets (see [Figure 4---figure supplement 4](#SD1){ref-type=\"supplementary-material\"}.\n",
      "\n",
      "              \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test code generation\n",
    "prompt2 = \"Write R code for differential expression analysis using limma package\"\n",
    "inputs2 = tokenizer(prompt2, return_tensors=\"pt\")\n",
    "outputs2 = model.generate(**inputs2, max_length=100, num_return_sequences=1, do_sample=True, top_p=0.95, temperature=0.8, pad_token_id=tokenizer.eos_token_id)\n",
    "result2 = tokenizer.decode(outputs2[0], skip_special_tokens=True)\n",
    "print(f\"Prompt: {prompt2}\")\n",
    "print(f\"Output: {result2}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

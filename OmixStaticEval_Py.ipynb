{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9ebdb20-bab4-4c98-9feb-d6e61c5feb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from openai import OpenAI\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "class LLMClient:\n",
    "    \"\"\"OpenAI API client wrapper class\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 api_key: str,\n",
    "                 base_url: str = \"https://api.openai.com/v1\",\n",
    "                 model: str = \"gpt-3.5-turbo\"):\n",
    "        \"\"\"\n",
    "        Initialize LLM client\n",
    "        \n",
    "        Args:\n",
    "            api_key: OpenAI API key\n",
    "            base_url: API base URL\n",
    "            model: Default model to use\n",
    "        \"\"\"\n",
    "        self.client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "        self.default_model = model\n",
    "    \n",
    "    def chat(self, \n",
    "             messages: list,\n",
    "             model: Optional[str] = None,\n",
    "             temperature: float = 0.7,\n",
    "             max_tokens: Optional[int] = None,\n",
    "             top_p: float = 1.0,\n",
    "             frequency_penalty: float = 0.0,\n",
    "             presence_penalty: float = 0.0,\n",
    "             stop: Optional[list] = None,\n",
    "             stream: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Send chat request\n",
    "        \n",
    "        Args:\n",
    "            messages: List of messages\n",
    "            model: Model to use, if None uses default model\n",
    "            temperature: Temperature parameter (0.0-2.0)\n",
    "            max_tokens: Maximum number of tokens\n",
    "            top_p: Top_p parameter\n",
    "            frequency_penalty: Frequency penalty\n",
    "            presence_penalty: Presence penalty\n",
    "            stop: List of stop words\n",
    "            stream: Whether to stream output\n",
    "            \n",
    "        Returns:\n",
    "            str: Model response content\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=model or self.default_model,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                top_p=top_p,\n",
    "                frequency_penalty=frequency_penalty,\n",
    "                presence_penalty=presence_penalty,\n",
    "                stop=stop,\n",
    "                stream=stream\n",
    "            )\n",
    "            \n",
    "            if stream:\n",
    "                return response  # Return generator object\n",
    "            else:\n",
    "                return response.choices[0].message.content\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"API call error: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "def CodeEval(task: str, response: str, llm_client: LLMClient) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Bioinformatics code evaluation function\n",
    "    \n",
    "    Args:\n",
    "        task: Task description to evaluate\n",
    "        response: R code response to evaluate\n",
    "        llm_client: LLM client instance\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Dictionary containing evaluation results\n",
    "    \"\"\"\n",
    "    \n",
    "    # Evaluation system prompt\n",
    "    system_prompt = \"\"\"You are an expert bioinformatics code reviewer with extensive experience in computational biology, omics, and biostatistics. Your task is to evaluate bioinformatics code solutions objectively and provide constructive feedback. Focus on scientific correctness, practical utility, and professional standards in bioinformatics.\"\"\"\n",
    "    \n",
    "    # Evaluation prompt template\n",
    "    evaluation_prompt_template = \"\"\"## Evaluation Instructions\n",
    "\n",
    "You will be provided with:\n",
    "1. A bioinformatics task/question\n",
    "2. R Code that attempts to solve this task\n",
    "\n",
    "Evaluate the R code using the framework below. This is a positive-scoring system - award points for good practices, do not subtract.\n",
    "\n",
    "## Scoring Framework (100 points total)\n",
    "\n",
    "### Part 1: Core Scientific Validity (45 points)\n",
    "\n",
    "#### 1.1 Problem Solving (20 points)\n",
    "- [ ] Code addresses the biological question (+10)\n",
    "- [ ] Solution approach is scientifically sound (+10)\n",
    "\n",
    "#### 1.2 Technical Implementation (25 points)\n",
    "- [ ] Appropriate methods for data type (+15)\n",
    "  - RNA-seq: DESeq2/edgeR/limma or proper transformation\n",
    "  - Genomics: GenomicRanges or appropriate coordinate handling\n",
    "  - Clustering: suitable algorithm for data characteristics\n",
    "  - General: method matches the biological question\n",
    "- [ ] Complete analysis workflow (+10)\n",
    "  - All necessary steps present\n",
    "  - Logical flow from input to conclusion\n",
    "\n",
    "### Part 2: Technical Quality (30 points)\n",
    "\n",
    "#### 2.1 Data Handling (15 points)\n",
    "- [ ] Appropriate preprocessing/normalization (+8)\n",
    "  - Log transformation for expression data\n",
    "  - Scaling for multi-omics integration\n",
    "  - Batch effect consideration\n",
    "- [ ] Data quality control (+7)\n",
    "  - Checks for NA/missing values\n",
    "  - Dimension validation\n",
    "  - Sample/feature filtering\n",
    "\n",
    "#### 2.2 Statistical Rigor (15 points)\n",
    "- [ ] Multiple testing correction when needed (+8)\n",
    "  - FDR/BH/Bonferroni for multiple comparisons\n",
    "  - **If not applicable (e.g., clustering only): award 8 points**\n",
    "- [ ] Appropriate statistical methods (+7)\n",
    "  - Correct test for data distribution\n",
    "  - Proper handling of paired/grouped data\n",
    "  - **If only visualization/processing: award 7 points**\n",
    "\n",
    "### Part 3: Professional Excellence (25 points)\n",
    "\n",
    "#### 3.1 Domain Knowledge & Interpretation (10 points)\n",
    "- [ ] Shows understanding of biological context (+5)\n",
    "  - Explains why methods suit the biological data\n",
    "  - Mentions relevant biological considerations\n",
    "- [ ] Results are biologically interpretable (+5)\n",
    "  - Output can be understood by biologists\n",
    "  - Includes relevant biological annotation\n",
    "\n",
    "#### 3.2 Robustness & Completeness (10 points)\n",
    "- [ ] Error handling or input validation (+5)\n",
    "  - Try-catch blocks or if-statements for edge cases\n",
    "  - Informative messages or warnings\n",
    "- [ ] Analysis validation or quality checks (+5)\n",
    "  - Parameter optimization (e.g., choosing k for clustering)\n",
    "  - Stability/reproducibility considerations (e.g., set.seed)\n",
    "\n",
    "#### 3.3 Documentation & Usability (5 points)\n",
    "- [ ] Clear workflow and outputs (+3)\n",
    "  - Can follow the analysis logic\n",
    "  - Results are saved or displayed\n",
    "- [ ] Adequate documentation (+2)\n",
    "  - Key steps explained\n",
    "  - Parameters justified\n",
    "\n",
    "## Evaluation Guidelines\n",
    "\n",
    "1. **Focus on scientific merit** - Correct biology > elegant code\n",
    "2. **Recognize good practices** - Award points for any professional elements\n",
    "3. **Consider context** - Simple working solutions can score well if scientifically sound\n",
    "\n",
    "## Task to evaluate:\n",
    "{task}\n",
    "\n",
    "## R Code to evaluate:\n",
    "{response}\n",
    "\n",
    "## Output Format\n",
    "\n",
    "Return ONLY the JSON below with no additional text:\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"total_score\": <integer 0-100>,\n",
    "  \"breakdown\": {{\n",
    "    \"problem_solving\": <integer 0-20>,\n",
    "    \"technical_implementation\": <integer 0-25>,\n",
    "    \"data_handling\": <integer 0-15>,\n",
    "    \"statistical_rigor\": <integer 0-15>,\n",
    "    \"domain_knowledge\": <integer 0-10>,\n",
    "    \"robustness\": <integer 0-10>,\n",
    "    \"documentation\": <integer 0-5>\n",
    "  }}\n",
    "}}\n",
    "```\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Build evaluation prompt\n",
    "        eval_prompt = evaluation_prompt_template.format(\n",
    "            task=task,\n",
    "            response=response\n",
    "        )\n",
    "        \n",
    "        # Build messages\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": eval_prompt}\n",
    "        ]\n",
    "        \n",
    "        # Call LLM for evaluation\n",
    "        evaluation_content = llm_client.chat(\n",
    "            messages=messages,\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        if evaluation_content is None:\n",
    "            return {\"error\": \"LLM API call failed\"}\n",
    "        \n",
    "        # Parse JSON response\n",
    "        try:\n",
    "            eval_result = json.loads(evaluation_content)\n",
    "            \n",
    "            # Calculate total score\n",
    "            total_score = eval_result.get(\"total_score\", 0)\n",
    "            breakdown = eval_result.get(\"breakdown\", {})\n",
    "            \n",
    "            # If no total_score, calculate from breakdown\n",
    "            if total_score == 0 and breakdown:\n",
    "                total_score = sum(score for score in breakdown.values() if isinstance(score, (int, float)))\n",
    "            \n",
    "            result = {\n",
    "                \"total_score\": total_score,\n",
    "                \"breakdown\": breakdown\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            # Try to extract JSON from response\n",
    "            json_pattern = r'\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}'\n",
    "            match = re.search(json_pattern, evaluation_content)\n",
    "            \n",
    "            if match:\n",
    "                try:\n",
    "                    eval_result = json.loads(match.group(0))\n",
    "                    total_score = eval_result.get(\"total_score\", 0)\n",
    "                    breakdown = eval_result.get(\"breakdown\", {})\n",
    "                    \n",
    "                    if total_score == 0 and breakdown:\n",
    "                        total_score = sum(score for score in breakdown.values() if isinstance(score, (int, float)))\n",
    "                    \n",
    "                    return {\n",
    "                        \"total_score\": total_score,\n",
    "                        \"breakdown\": breakdown\n",
    "                    }\n",
    "                except json.JSONDecodeError:\n",
    "                    pass\n",
    "            \n",
    "            print(f\"JSON parsing failed: {evaluation_content}\")\n",
    "            return {\"error\": \"JSON parsing failed\"}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d3c161-5dd0-4fae-9ef8-6cf0a6dc7874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
